{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "In this notebook, I'll be capturing all the essential information that was provided on the lectures. Each definition and\n",
    "algorithm will be researched and provided with an example.\n",
    "\n",
    "Many applications involve temporal dependencies over time. It means that our current output depends not only on the\n",
    "current input but also past ones (unlike feedforward approach). The neural network architectures you've seen so far were trained using the current inputs only. We did not consider\n",
    "previous inputs when generating the current output. In other words, our systems did not have any memory elements.\n",
    "RNNs address this very basic and important issue by using memory (i.e. past inputs to the network) when producing the\n",
    "current output.\n",
    "* temporal dependencies - dependencies that change over time (e.g., gifs, videos)\n",
    "\n",
    "![](images/tdnn.png)\n",
    "\n",
    "### Feedforward Neural networks\n",
    "* Nonlinear function approximations\n",
    "* Training (backpropagation & SGD)\n",
    "* Evaluation\n",
    "\n",
    "### Topics to be covered in Recurrent Neural Networks\n",
    "* Applications\n",
    "* Simple RNN Elman Network\n",
    "* Training RNNs\n",
    "* LSTM - Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### History of RNN\n",
    "1. Time delay Neural Networks (TDNN 1989)\n",
    "Inputs from past timestamps were introduced to the network changing the actual external inputs. Disadvantage: they had limited to the window of time chosen.\n",
    "2. Simple RNN / Elman Network (1990)\n",
    "3. Long Short-Term Memory (LSTM mid 1990s)\n",
    "\n",
    "### RNN's biggest flaws\n",
    "RNNs have a key flaw, as capturing relationships that span more than 8 or 10 steps back is practically impossible.\n",
    "This flaw stems from the \"vanishing gradient\" problem in which the contribution of information decays geometrically over time.\n",
    "As you may recall, while training our network we use backpropagation. In the backpropagation process we adjust our weight matrices with the use of a gradient. In the process, gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically \"vanish\".\n",
    "\n",
    "#### How to overcome that?\n",
    "* LSTM\n",
    "* Faster Hardware\n",
    "* Residual Networks: skipping (residual) connections as part of the network architecture (ResNets). They yield lower training error by reintroducing outputs from shallower layers in the network to compensate the vanishing data. (However, this doesn't solve the fundamental problem, it only avoids it.)\n",
    "* Other Activation Functions: ReLU suffers less from vanishing gradients (they saturate in one direction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Papers to check out\n",
    "* <a href=\"https://arxiv.org/pdf/1404.7828.pdf\">Deep Learning in NN</a>\n",
    "* <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1\">Elman Network original paper</a>\n",
    "* <a href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\">LSTM original paper</a>\n",
    "* <a href=\"https://arxiv.org/pdf/1511.06939.pdf\">Netflix Recommendations</a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applications of RNN and LSTM\n",
    "- Speech Recognition, NLP & Chatbots\n",
    "- Time Series prediction: traffic patterns, movie selection, stock movements\n",
    "- Gesture Recognition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combining CNN with RNN\n",
    "![](images/rnn.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}